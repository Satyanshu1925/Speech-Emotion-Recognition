{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5d474e",
   "metadata": {},
   "source": [
    "# SPEECH EMOTION RECOGNITION SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b1e26",
   "metadata": {},
   "source": [
    "**PROJECT OVERVIEW**\n",
    "\n",
    "Through all the available senses humans can actually sense the emotional state of their communication partner. The emotional detection is natural for humans but it is very difficult task for computers; although they can easily understand content based information, accessing the depth behind content is difficult and that’s what speech emotion recognition (SER) sets out to do. It is a system through which various audio speech files are classified into different emotions such as happy, sad, anger and neutral by computer. SER can be used in areas such as the medical field or customer call centers.\n",
    "\n",
    "**DATASET**\n",
    "\n",
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) Dataset from Kaggle contains 1440 audio files from 24 Actors vocalizing two lexically-matched statements. Emotions include angry, happy, sad, fearful, calm, neutral, disgust, and surprised.[Dataset](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c35345",
   "metadata": {},
   "source": [
    "### STEP 1 - IMPORT DEPENDENCIES LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a718613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install all the Reqiuired Libraries and Packages \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc , logfbank\n",
    "import pickle\n",
    "import librosa\n",
    "from scipy import signal\n",
    "import noisereduce as nr\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "import soundfile\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e087e2",
   "metadata": {},
   "source": [
    "### STEP 2 - LOAD THE RAVDESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff7721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the required RAVDESS DataSet with length of 1439 Audio Files \n",
    "os.listdir(path='./speech-emotion-recognition-ravdess-data')\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile=os.listdir(dirName)\n",
    "    allFiles=list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath=os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles=allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "\n",
    "dirName = './speech-emotion-recognition-ravdess-data'\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "len(listOfFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2600d1",
   "metadata": {},
   "source": [
    "**Male Neutral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Male Neutral\n",
    "\n",
    "# LOAD IN FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_01/03-01-01-01-01-01-01.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Male Neutral')\n",
    "plt.savefig('Waveplot_MaleNeutral.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_01/03-01-01-01-01-01-01.wav', x,sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21703d",
   "metadata": {},
   "source": [
    "**Female Calm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Female Calm\n",
    "\n",
    "# LOAD IN FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_02/03-01-02-01-01-01-02.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Female Calm')\n",
    "plt.savefig('Waveplot_FemaleCalm.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_02/03-01-02-01-01-01-02.wav', x,sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fd3e7",
   "metadata": {},
   "source": [
    "**Male Happy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Male Happy\n",
    "\n",
    "# LOAD IN FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_03/03-01-03-01-01-01-03.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Male Happy')\n",
    "plt.savefig('Waveplot_MaleHappy.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_03/03-01-03-01-01-01-03.wav', x,sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd73b9c",
   "metadata": {},
   "source": [
    "**Female Sad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Female Sad\n",
    "\n",
    "# LOAD FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_04/03-01-04-01-01-01-04.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Female Sad')\n",
    "plt.savefig('Waveplot_FemaleSad.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_04/03-01-04-01-01-01-04.wav', x, sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c290b",
   "metadata": {},
   "source": [
    "**Male Angry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Male Angry\n",
    "\n",
    "# LOAD FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_05/03-01-05-02-02-01-05.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Male Angry')\n",
    "plt.savefig('Waveplot_MaleAngry.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_05/03-01-05-01-01-01-02.wav', x, sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7c8e1",
   "metadata": {},
   "source": [
    "**Female Fearful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Female Fearful\n",
    "\n",
    "# LOAD FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_06/03-01-06-01-01-01-06.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Female Fearful')\n",
    "plt.savefig('Waveplot_FemaleFearful.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_06/03-01-06-01-01-01-06.wav', x, sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3fcfe",
   "metadata": {},
   "source": [
    "**Male Disgust**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Male Disgust\n",
    "\n",
    "# LOAD FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_07/03-01-07-01-01-01-07.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Male Disgust')\n",
    "plt.savefig('Waveplot_MaleDisgust.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_07/03-01-07-01-01-01-07.wav', x, sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7ba2d",
   "metadata": {},
   "source": [
    "**Female Surprised**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Female Surprised\n",
    "\n",
    "# LOAD FILE\n",
    "x, sr = librosa.load('./speech-emotion-recognition-ravdess-data/Actor_08/03-01-08-01-01-01-08.wav')\n",
    "\n",
    "# DISPLAY WAVEPLOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.waveshow(x, sr=sr)\n",
    "plt.title('Waveplot - Female Surprised')\n",
    "plt.savefig('Waveplot_FemaleSurprised.png')\n",
    "\n",
    "# PLAY AUDIO FILE\n",
    "sf.write('./speech-emotion-recognition-ravdess-data/Actor_08/03-01-08-01-01-01-08.wav', x, sr)\n",
    "Audio(data=x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c47ac4",
   "metadata": {},
   "source": [
    "### STEP 3 - USING SPEECH RECOGNITION API TO CONVERT AUDIO TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb309687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104    dogs sitting by the door\n",
      "105    kids are talking by the door\n",
      "106    sitting by the door\n",
      "107    dogs sitting by the door\n",
      "108    Khesari talking by the door\n",
      "109    talking by the door\n"
     ]
    }
   ],
   "source": [
    "#Use the Speech-Recognition API to get the Raw Text from Audio Files, Though Speech Recognition\n",
    "#is less strong for large chunk of files , so used Error Handling , where when it is not be able to \n",
    "#produce the text of a particular Audio File it prints the statement 'error'.Just for understanding Audio\n",
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "\n",
    "# for file in range(0 , 10 , 1):\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    with sr.AudioFile(listOfFiles[file]) as source:\n",
    "        audio = r.record(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(file,\"  \",text)\n",
    "        except:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe56acf",
   "metadata": {},
   "source": [
    "### STEP 4 - PLOTTING TO UNDERSTAND RAW AUDIO FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5972efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a664d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Basic Graphs for understanding of Audio Files :\n",
    "# for file in range(0 , 10 , 1):\n",
    "for file in range(0, len(listOfFiles), 1):\n",
    "    print(file + 1)\n",
    "    audio , sfreq = librosa.load(listOfFiles[file])\n",
    "    time = np.arange(0 , len(audio)) / sfreq\n",
    "    \n",
    "    fig ,ax = plt.subplots()\n",
    "    ax.plot(time , audio)\n",
    "    ax.set(xlabel = 'Time (s)' , ylabel = 'Sound Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "#PLOT THE SEPCTOGRAM\n",
    "# for file in range(0 , 10 , 1):\n",
    "for file in range(0, len(listOfFiles), 1):\n",
    "    print(file+1)\n",
    "    sample_rate , samples = wavfile.read(listOfFiles[file])\n",
    "    frequencies , times, spectrogram = signal.spectrogram(samples, sample_rate) \n",
    "    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "    plt.imshow(spectrogram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee682afb",
   "metadata": {},
   "source": [
    "### STEP 5 - VISUALIZATION OF AUDIO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Step is In-Depth Visualisation of Audio Fiels and its certain features to plot for.\n",
    "#They are the Plotting Functions to be called later. \n",
    "def plot_signals(signals):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Time Series' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(signals.keys())[i])\n",
    "            axes[x,y].plot(list(signals.values())[i])\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_fft(fft):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Fourier Transform' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            data = list(fft.values())[i]\n",
    "            Y,freq = data[0] , data[1]\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq , Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "    \n",
    "def plot_fbank(fbank):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Filter Bank Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(list(fbank.values())[i],cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "            \n",
    "def plot_mfccs(mfccs):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Mel Frequency Capstrum  Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x,y].imshow(list(mfccs.values())[i],\n",
    "                             cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def calc_fft(y,rate):\n",
    "    n = len(y)\n",
    "    freq = np.fft.rfftfreq(n , d= 1/rate)\n",
    "    Y= abs(np.fft.rfft(y)/n)\n",
    "    return(Y,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here The Data Set is loaded and plots are Visualised by Calling the Plotting Functions . \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "import numpy as np\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    rate, data = wav.read(listOfFiles[file])\n",
    "    fft_out = fft(data)\n",
    "    %matplotlib inline\n",
    "    plt.plot(data, np.abs(fft_out))\n",
    "    plt.show()\n",
    "    \n",
    "signals={}\n",
    "fft={}\n",
    "fbank={}\n",
    "mfccs={}\n",
    "# load data\n",
    "# for file in range(0 , len(listOfFiles) , 1):\n",
    "for file in range(0 , 10 , 1):\n",
    "#     rate, data = wavfile.read(listOfFiles[file])\n",
    "     signal,rate =librosa.load(listOfFiles[file] , sr=44100)\n",
    "     mask = envelope(signal , rate , 0.0005)\n",
    "     signals[file] = signal\n",
    "     fft[file] = calc_fft(signal , rate)\n",
    "    \n",
    "     bank = logfbank(signal[:rate] , rate , nfilt = 26, nfft = 1103).T\n",
    "     fbank[file] = bank\n",
    "     mel = mfcc(signal[:rate] , rate , numcep =13 , nfilt = 26 , nfft=1103).T\n",
    "     mfccs[file]=mel\n",
    "\n",
    "plot_signals(signals)\n",
    "plt.show()\n",
    "\n",
    "plot_fft(fft)\n",
    "plt.show()\n",
    "\n",
    "plot_fbank(fbank)\n",
    "plt.show()\n",
    "\n",
    "plot_mfccs(mfccs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52822c8b",
   "metadata": {},
   "source": [
    "### STEP 6 - CLEANING & MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Cleaning Step is Performed where:\n",
    "#DOWN SAMPLING OF AUDIO FILES IS DONE  AND PUT MASK OVER IT AND DIRECT INTO CLEAN FOLDER\n",
    "#MASK IS TO REMOVE UNNECESSARY EMPTY VOIVES AROUND THE MAIN AUDIO VOICE \n",
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The clean Audio Files are redirected to Clean Audio Folder Directory \n",
    "import glob,pickle\n",
    "for file in tqdm(glob.glob(r'./speech-emotion-recognition-ravdess-data/**/*.wav')):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal , rate = librosa.load(file, sr=16000)\n",
    "    mask = envelope(signal,rate, 0.0005)\n",
    "    wavfile.write(filename= r'./clean_speech/'+str(file_name), rate=rate,data=signal[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a876d",
   "metadata": {},
   "source": [
    "### STEP 7 - FEATURE EXTRACTION\n",
    "\n",
    "Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "\n",
    "**mfcc:** Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "\n",
    "**chroma:** Pertains to the 12 different pitch classes\n",
    "\n",
    "**mel: Mel Spectrogram Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction of Audio Files Function \n",
    "#Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1da585",
   "metadata": {},
   "source": [
    "### STEP 8 - LABELS CLASSIFICATION\n",
    "\n",
    "Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want- calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1138a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions in the RAVDESS dataset to be classified Audio Files based on . \n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "#These are the emotions User wants to observe more :\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffba40",
   "metadata": {},
   "source": [
    "### STEP 9 - LOADING OF DATA & SPLITTING OF DATASET\n",
    "\n",
    "Now, let’s load the clean data with a function load_data() – this takes in the relative size of the test set as parameter. x and y are empty lists; we’ll use the glob() function from the glob module to get all the pathnames for the sound files in our dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ff0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the clean data\n",
    "from glob import glob\n",
    "import os\n",
    "import glob\n",
    "def load_data(test_size=0.33):\n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(r'./clean_speech/*.wav'):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            answer += 1\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion,file_name])\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449ee31",
   "metadata": {},
   "source": [
    "Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c793bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "x_train,x_test,y_trai,y_tes=load_data(test_size=0.25)\n",
    "print(np.shape(x_train),np.shape(x_test), np.shape(y_trai),np.shape(y_tes))\n",
    "y_test_map = np.array(y_tes).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_trai).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train),np.shape(y_test))\n",
    "print(*test_filename,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c019dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "# print((x_train.shape[0], x_test.shape[0]))\n",
    "print((x_train[0], x_test[0]))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793e0cb",
   "metadata": {},
   "source": [
    "### STEP 10 - APPLY MLP CLASSIFIER\n",
    "\n",
    "Now, let’s Apply a MLPClassifier. This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using LBFGS or stochastic gradient descent. Unlike SVM or Naive Bayes, the MLPClassifier has an internal neural network for the purpose of classification. This is a feedforward ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95131803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7315ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cd766",
   "metadata": {},
   "source": [
    "### STEP 11 - SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "import pickle\n",
    "# Save the Modle to file in the current working directory\n",
    "#For any new testing data other than the data in dataset\n",
    "\n",
    "Pkl_Filename = \"Speech_Emotions_Recognition_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197f1de",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### STEP 11 - LOAD THE SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5286527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Speech_Emotions_Recognition_Model = pickle.load(file)\n",
    "\n",
    "Speech_Emotions_Recognition_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7f7f3",
   "metadata": {},
   "source": [
    "### STEP 12 - PREDICT THE TEST DATA USING THE SAVED MODEL\n",
    "\n",
    "Let’s predict the values for the test set from saved model. This gives us y_pred (the predicted emotions for the features in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting :\n",
    "y_pred=Speech_Emotions_Recognition_Model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech_Emotions_Recognition_Model.predict([[-3.30682434e+02,  4.68944969e+01, -2.71848297e+01,  1.24464293e+01,\n",
    "       -3.28550949e+01, -2.91180630e+01, -3.37174377e+01, -1.46248970e+01,\n",
    "       -9.23727608e+00, -1.89770374e+01, -1.87777061e+01, -3.96805000e+00,\n",
    "       -2.03033276e+01, -1.67429709e+00, -1.89696198e+01, -6.77547359e+00,\n",
    "       -2.12655239e+01, -8.34370136e+00, -9.58549786e+00,  2.27216721e+00,\n",
    "        3.32305884e+00,  1.10515509e+01, -1.85491896e+00,  1.12577772e+01,\n",
    "        1.08444405e+00,  1.28305206e+01,  3.76580906e+00,  7.26798439e+00,\n",
    "       -4.22768402e+00, -9.67685699e-01, -3.59515381e+00,  4.46669579e+00,\n",
    "        3.29391098e+00,  2.68758559e+00, -5.97882366e+00,  3.29751945e+00,\n",
    "        4.02787495e+00,  2.78433490e+00, -4.19763231e+00, -4.04090214e+00,\n",
    "        4.72516418e-01,  4.64918762e-01,  4.07587588e-01,  3.87473792e-01,\n",
    "        4.25525159e-01,  4.69874412e-01,  4.68603343e-01,  4.33787495e-01,\n",
    "        5.26395321e-01,  5.43187857e-01,  6.11121118e-01,  5.85302591e-01,\n",
    "        1.99202223e-05,  1.14141085e-05,  3.96283649e-05,  7.53003551e-05,\n",
    "        9.73876959e-05,  9.64810431e-04,  4.69703302e-02,  8.64494815e-02,\n",
    "        2.52236009e-01,  8.18835199e-01,  1.51921189e+00,  1.40999377e+00,\n",
    "        4.93527502e-01,  1.02135277e+00,  1.54811442e+00,  2.13874429e-01,\n",
    "        1.38695091e-01,  1.55450314e-01,  3.68789613e-01,  1.76540029e+00,\n",
    "        3.55493307e+00,  1.81521642e+00,  4.16990936e-01,  9.83263195e-01,\n",
    "        8.63637507e-01,  8.46345663e-01,  6.87963188e-01,  2.48230085e-01,\n",
    "        4.05039400e-01,  3.50113422e-01,  4.47095037e-01,  1.08914554e+00,\n",
    "        6.89291298e-01,  1.08541094e-01,  4.36025783e-02,  8.25937748e-01,\n",
    "        1.91191185e+00,  4.79927838e-01,  6.15618527e-01,  8.51190388e-01,\n",
    "        8.33118081e-01,  2.29535371e-01,  5.69139235e-02,  1.61346011e-02,\n",
    "        2.96809082e-03,  1.28785614e-03,  1.61191355e-02,  9.04617757e-02,\n",
    "        1.22349039e-01,  1.35659382e-01,  6.49800673e-02,  6.52751327e-02,\n",
    "        1.01899616e-01,  3.19572426e-02,  2.44785920e-02,  6.54902831e-02,\n",
    "        2.80245215e-01,  7.96612725e-02,  4.53595072e-02,  4.95095514e-02,\n",
    "        3.30882519e-02,  5.24889529e-02,  7.43068457e-02,  1.18114151e-01,\n",
    "        5.18677644e-02,  3.36263552e-02,  2.03397907e-02,  2.76837200e-02,\n",
    "        2.37510055e-02,  3.53552625e-02,  7.46268779e-02,  5.56919686e-02,\n",
    "        1.74056310e-02,  1.60045661e-02,  1.76128838e-02,  1.52157638e-02,\n",
    "        2.37508211e-02,  1.80982929e-02,  3.16780210e-02,  2.38330122e-02,\n",
    "        1.18345730e-02,  1.05736358e-02,  1.21288951e-02,  5.26925456e-03,\n",
    "        9.89388488e-03,  2.72622090e-02,  1.50585957e-02,  8.23779497e-03,\n",
    "        9.10799578e-03,  1.47639308e-02,  1.00468704e-02,  8.04410502e-03,\n",
    "        2.01563500e-02,  1.84375662e-02,  2.99732536e-02,  1.34403706e-02,\n",
    "        2.95010451e-02,  1.65024269e-02,  1.56056313e-02,  3.85226905e-02,\n",
    "        3.28611434e-02,  1.78700201e-02,  1.21603282e-02,  4.07257257e-03,\n",
    "        2.12775078e-03,  1.64698774e-03,  1.00917544e-03,  1.50620006e-03,\n",
    "        1.16550620e-03,  1.81205489e-03,  1.73890230e-03,  2.22400110e-03,\n",
    "        2.66271620e-03,  2.36251648e-03,  2.89487024e-03,  4.90537193e-03,\n",
    "        7.22360564e-03,  9.44759045e-03,  8.14894494e-03,  8.75739288e-03,\n",
    "        1.08842561e-02,  8.98819603e-03,  1.53665729e-02,  1.18999509e-02,\n",
    "        5.41199371e-03,  1.88657362e-03,  9.39109668e-05,  2.60356887e-06]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f6641",
   "metadata": {},
   "source": [
    "### STEP 13 - SUMMARIZATION OF PREDICTED DATA\n",
    "\n",
    "To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "results = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(results)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ac8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57781e2e",
   "metadata": {},
   "source": [
    "### STEP 14 - STORE THE PREDICTED FILE IN .CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the Prediction probabilities into CSV file \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "y_pred1['file_names'] = test_filename\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv('predictionfinal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a2544",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3eba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c3fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
